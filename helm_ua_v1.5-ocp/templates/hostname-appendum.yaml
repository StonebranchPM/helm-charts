{{- if .Values.tlssnihostnameref.enabled }}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ .Release.Name }}-cm-patcher
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-install,post-upgrade
    "helm.sh/hook-weight": "3"
    "helm.sh/hook-delete-policy": before-hook-creation
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: {{ .Release.Name }}-cm-patcher
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-install,post-upgrade
    "helm.sh/hook-weight": "3"
    "helm.sh/hook-delete-policy": before-hook-creation
rules:
  - apiGroups: ["route.openshift.io"]
    resources: ["routes"]
    verbs: ["get","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get","patch","update"]
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get","patch","update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{ .Release.Name }}-cm-patcher
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-install,post-upgrade
    "helm.sh/hook-weight": "3"
    "helm.sh/hook-delete-policy": before-hook-creation
subjects:
  - kind: ServiceAccount
    name: {{ .Release.Name }}-cm-patcher
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: {{ .Release.Name }}-cm-patcher
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-ua-route-cm-patch
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-install,post-upgrade
    "helm.sh/hook-weight": "10"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  backoffLimit: 2
  template:
    spec:
      serviceAccountName: {{ .Release.Name }}-cm-patcher
      restartPolicy: Never
      containers:
        - name: patcher
          image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest  # <â€” built-in image
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash","-lc"]
          env:
            - name: NS
              value: "{{ .Release.Namespace }}"
            - name: ROUTE_NAME
              value: "{{ .Release.Name }}-udm-route"
            - name: CONFIGMAP_NAME
              value: "{{ .Release.Name }}-uasni-configmap"
            - name: DEPLOYMENT_NAME
              value: "{{ .Release.Name }}-ua"
          args:
            - |
              set -euo pipefail
              echo "Waiting for Route ${ROUTE_NAME} in ${NS} to be Admitted..."
              for i in {1..180}; do
                HOST="$(kubectl -n "${NS}" get route "${ROUTE_NAME}" -o jsonpath='{.status.ingress[0].host}' 2>/dev/null || true)"
                ADMIT="$(kubectl -n "${NS}" get route "${ROUTE_NAME}" -o jsonpath='{.status.ingress[0].conditions[?(@.type=="Admitted")].status}' 2>/dev/null || true)"
                if [[ -n "${HOST}" && "${ADMIT}" == "True" ]]; then
                  echo "Route admitted with host: ${HOST}"
                  break
                fi
                sleep 2
              done
              if [[ -z "${HOST:-}" ]]; then
                echo "ERROR: Route host not available"; exit 1
              fi

              SNI="443@${HOST}:${HOST}"
              echo "Patching ${CONFIGMAP_NAME} data.UAGTLSSNIHOSTNAME=${SNI}"
              kubectl -n "${NS}" patch configmap "${CONFIGMAP_NAME}" --type merge \
                -p "{\"data\":{\"UAGTLSSNIHOSTNAME\":\"${SNI}\"}}"

              {{- if .Release.IsUpgrade }}
              echo "Post-upgrade: restarting deployment ${DEPLOYMENT_NAME}"
              kubectl -n "${NS}" patch deployment "${DEPLOYMENT_NAME}" --type='merge' \
                -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -Iseconds)\"}}}}}"
              {{- end }}
{{- end }}
